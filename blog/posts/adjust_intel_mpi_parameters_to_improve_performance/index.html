<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.83.1" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="石京昶" />
  <meta property="og:url" content="https://shijingchang.cn/blog/posts/adjust_intel_mpi_parameters_to_improve_performance/" />
  <link rel="canonical" href="https://shijingchang.cn/blog/posts/adjust_intel_mpi_parameters_to_improve_performance/" /><link rel="alternate" type="application/atom+xml" href="https://shijingchang.cn/blogindex.xml" title="石京昶的博客">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/shijingchang.cn\/blog"
      },
      "articleSection" : "posts",
      "name" : "调整Intel MPI 2018中的参数提升通信性能",
      "headline" : "调整Intel MPI 2018中的参数提升通信性能",
      "description" : "概览 前些天在超算上跑我的程序，结果遇到了一些MPI相关问题，这里记录我解决（某种程度上）这个问题的过程。\n背景 去年12月-今年5月，我使用B地超算，跑我自己写的纯MPI并行程序，使用mpi\/Intel\/20.4.3，1024核以下没有出现过启动不了的情况。 当时1024核常常无法启动MPI_Init，管理员当时承认确实有问题但无法解决。 后续我一直在用mpi\/intel\/18.0.2。\n8月我再次开始使用该超算，mpi\/intel\/18.0.2跑我的纯MPI并行程序。 但是出现了512核也时常MPI collective IO卡死的问题。 月初无法解决此问题，于是尝试该地超算其它分区以及J地的超算。\n解决 J地超算上提交几次1000\u002b核任务均无法启动MPI_Init，遂放弃。\nB地超算上新分区节点任务繁忙，1000\u002b核的任务等待时间较长，所以我没有太多测试。 但几次测试时常出现运行时MPI卡死。 经过阅读MPI相关资料，发现在Intel MPI 18版本下添加环境变量I_MPI_DAPL_UD=on启动UD模式通信可有效规避此问题。 回到B地原分区下，添加此环境变量后同样使得MPI卡死的问题几乎消失。 Intel官方文档解释：UD模式有效减少了MPI通信send\/recv所需的缓冲区，显著降低了MPI占用的内存。 所以之前遇到的1000核任务运行时MPI卡死的现象应该是与内存相关。\n鉴于三个分区的表现没有本质差别，我又回到了B地原分区尝试解决MPI问题。 该分区采用mpi\/intel\/18.0.2，设定以下环境变量\nI_MPI_FABRICS=shm:dapl I_MPI_DAPL_UD=on I_MPI_FALLBACK=off 启动1024核任务，MPI init启动问题几乎消失了，MPI IO卡死问题几乎消失了。 但我的程序性能出现了巨大差异。 有时候1024核任务开始的500步迭代速度还可以，但随后变慢为1\/3速度。有时候则一开始迭代速度就是预期的1\/3慢。\n我首先尝试理解内存的事情。 我首先给我的纯MPI程序添加了OpenMP directives，使其可以MPI\/OpenMP混合并行。 然后测试了相同问题规模和相同输入参数下的内存占用，1024核任务。 纯MPI并行，调用了16个节点，每个节点64个MPI processes。 程序内部通过查看\/proc\/self\/statm获知1024核总内存占用量为1.81E6 MB； MPI\/OpenMP混合并行，调用16个节点，每个节点32个MPI processes，每个MPI process开2个threads，程序内部查看\/proc\/self\/statm获知1024核总内存占用量为1.05E6 MB。 可以看到MPI并行本身的确占用了海量内存，几乎总内存占用量的一半都是MPI本身占用的。（这个事实，专业人员应该是了解的；我是逐渐认识到的，尝试解决问题。）\n另外我检查了我的程序是否存在内存泄漏。 MPI模式的valgrind显示在调用的MPI calls中存在memory lost，鉴于Intel MPI的广泛使用，我认为这是false positive。valgrind没有找到我程序中的内存泄漏。\n接下来，我尝试使用Intel自带的Intel Tracer And Collector来查看MPI性能。 但是按照Intel官方给的步骤加载ITAC，无法启动MPI。 所以为了理解我程序的MPI函数调用情况，我给我的程序手动添加了记录MPI调用时间的代码，每个进程记录自己的时间，不做额外的MPI call。 于是我分别提交了两次1024核纯MPI并行任务，得到了以下两图。 上图是前500步内程序运行较快时，单次迭代内1024核各自调用关键函数时的时间；下图则是程序慢3倍时，单次迭代内1024核各自调用关键函数时的时间。\n上图中程序运行前500步内较快，但存在1024核负载不均衡、MPI通信部分节点延迟，但并不严重。 橘色线到绿色线是程序核心的MPI调用，也就是MPI_Isend\/Irecv。 右端的粉红线到灰色线同样调用了MPI_Isend\/Irecv。 下图中显然1024核中某些节点的MPI通信延迟非常高，而且均发生在调用MPI_Isend\/Irecv的过程，也就是橘色线到绿色线、粉红线到灰色线。 所以1024核程序运行慢3倍的原因是某些节点MPI通信有问题，响应慢。\n于是我测试了Intel自带的IMB-MPI1 Exchange。这个MPI Benchmark模式Exchange测试的是MPI_Isend\/recv。 多次测试显示MPI_Isend\/recv从128核，也就是2个节点，就出现性能极其严重的下降。 我理解64核1个节点采用shared memory模式，带宽就是会很高，但IB网络下节点间通信带宽不应该过分差。下见表格。",
      "inLanguage" : "en-US",
      "author" : "石京昶",
      "creator" : "石京昶",
      "publisher": "石京昶",
      "accountablePerson" : "石京昶",
      "copyrightHolder" : "石京昶",
      "copyrightYear" : "2021",
      "datePublished": "2021-10-01 15:29:51 \u002b0800 CST",
      "dateModified" : "2021-10-01 15:29:51 \u002b0800 CST",
      "url" : "https:\/\/shijingchang.cn\/blog\/posts\/adjust_intel_mpi_parameters_to_improve_performance\/",
      "keywords" : [  ]
  }
</script>
<title>调整Intel MPI 2018中的参数提升通信性能</title>
  <meta property="og:title" content="调整Intel MPI 2018中的参数提升通信性能" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="概览 前些天在超算上跑我的程序，结果遇到了一些MPI相关问题，这里记录我解决（某种程度上）这个问题的过程。
背景 去年12月-今年5月，我使用B地超算，跑我自己写的纯MPI并行程序，使用mpi/Intel/20.4.3，1024核以下没有出现过启动不了的情况。 当时1024核常常无法启动MPI_Init，管理员当时承认确实有问题但无法解决。 后续我一直在用mpi/intel/18.0.2。
8月我再次开始使用该超算，mpi/intel/18.0.2跑我的纯MPI并行程序。 但是出现了512核也时常MPI collective IO卡死的问题。 月初无法解决此问题，于是尝试该地超算其它分区以及J地的超算。
解决 J地超算上提交几次1000&#43;核任务均无法启动MPI_Init，遂放弃。
B地超算上新分区节点任务繁忙，1000&#43;核的任务等待时间较长，所以我没有太多测试。 但几次测试时常出现运行时MPI卡死。 经过阅读MPI相关资料，发现在Intel MPI 18版本下添加环境变量I_MPI_DAPL_UD=on启动UD模式通信可有效规避此问题。 回到B地原分区下，添加此环境变量后同样使得MPI卡死的问题几乎消失。 Intel官方文档解释：UD模式有效减少了MPI通信send/recv所需的缓冲区，显著降低了MPI占用的内存。 所以之前遇到的1000核任务运行时MPI卡死的现象应该是与内存相关。
鉴于三个分区的表现没有本质差别，我又回到了B地原分区尝试解决MPI问题。 该分区采用mpi/intel/18.0.2，设定以下环境变量
I_MPI_FABRICS=shm:dapl I_MPI_DAPL_UD=on I_MPI_FALLBACK=off 启动1024核任务，MPI init启动问题几乎消失了，MPI IO卡死问题几乎消失了。 但我的程序性能出现了巨大差异。 有时候1024核任务开始的500步迭代速度还可以，但随后变慢为1/3速度。有时候则一开始迭代速度就是预期的1/3慢。
我首先尝试理解内存的事情。 我首先给我的纯MPI程序添加了OpenMP directives，使其可以MPI/OpenMP混合并行。 然后测试了相同问题规模和相同输入参数下的内存占用，1024核任务。 纯MPI并行，调用了16个节点，每个节点64个MPI processes。 程序内部通过查看/proc/self/statm获知1024核总内存占用量为1.81E6 MB； MPI/OpenMP混合并行，调用16个节点，每个节点32个MPI processes，每个MPI process开2个threads，程序内部查看/proc/self/statm获知1024核总内存占用量为1.05E6 MB。 可以看到MPI并行本身的确占用了海量内存，几乎总内存占用量的一半都是MPI本身占用的。（这个事实，专业人员应该是了解的；我是逐渐认识到的，尝试解决问题。）
另外我检查了我的程序是否存在内存泄漏。 MPI模式的valgrind显示在调用的MPI calls中存在memory lost，鉴于Intel MPI的广泛使用，我认为这是false positive。valgrind没有找到我程序中的内存泄漏。
接下来，我尝试使用Intel自带的Intel Tracer And Collector来查看MPI性能。 但是按照Intel官方给的步骤加载ITAC，无法启动MPI。 所以为了理解我程序的MPI函数调用情况，我给我的程序手动添加了记录MPI调用时间的代码，每个进程记录自己的时间，不做额外的MPI call。 于是我分别提交了两次1024核纯MPI并行任务，得到了以下两图。 上图是前500步内程序运行较快时，单次迭代内1024核各自调用关键函数时的时间；下图则是程序慢3倍时，单次迭代内1024核各自调用关键函数时的时间。
上图中程序运行前500步内较快，但存在1024核负载不均衡、MPI通信部分节点延迟，但并不严重。 橘色线到绿色线是程序核心的MPI调用，也就是MPI_Isend/Irecv。 右端的粉红线到灰色线同样调用了MPI_Isend/Irecv。 下图中显然1024核中某些节点的MPI通信延迟非常高，而且均发生在调用MPI_Isend/Irecv的过程，也就是橘色线到绿色线、粉红线到灰色线。 所以1024核程序运行慢3倍的原因是某些节点MPI通信有问题，响应慢。
于是我测试了Intel自带的IMB-MPI1 Exchange。这个MPI Benchmark模式Exchange测试的是MPI_Isend/recv。 多次测试显示MPI_Isend/recv从128核，也就是2个节点，就出现性能极其严重的下降。 我理解64核1个节点采用shared memory模式，带宽就是会很高，但IB网络下节点间通信带宽不应该过分差。下见表格。" />
  <meta name="description" content="概览 前些天在超算上跑我的程序，结果遇到了一些MPI相关问题，这里记录我解决（某种程度上）这个问题的过程。
背景 去年12月-今年5月，我使用B地超算，跑我自己写的纯MPI并行程序，使用mpi/Intel/20.4.3，1024核以下没有出现过启动不了的情况。 当时1024核常常无法启动MPI_Init，管理员当时承认确实有问题但无法解决。 后续我一直在用mpi/intel/18.0.2。
8月我再次开始使用该超算，mpi/intel/18.0.2跑我的纯MPI并行程序。 但是出现了512核也时常MPI collective IO卡死的问题。 月初无法解决此问题，于是尝试该地超算其它分区以及J地的超算。
解决 J地超算上提交几次1000&#43;核任务均无法启动MPI_Init，遂放弃。
B地超算上新分区节点任务繁忙，1000&#43;核的任务等待时间较长，所以我没有太多测试。 但几次测试时常出现运行时MPI卡死。 经过阅读MPI相关资料，发现在Intel MPI 18版本下添加环境变量I_MPI_DAPL_UD=on启动UD模式通信可有效规避此问题。 回到B地原分区下，添加此环境变量后同样使得MPI卡死的问题几乎消失。 Intel官方文档解释：UD模式有效减少了MPI通信send/recv所需的缓冲区，显著降低了MPI占用的内存。 所以之前遇到的1000核任务运行时MPI卡死的现象应该是与内存相关。
鉴于三个分区的表现没有本质差别，我又回到了B地原分区尝试解决MPI问题。 该分区采用mpi/intel/18.0.2，设定以下环境变量
I_MPI_FABRICS=shm:dapl I_MPI_DAPL_UD=on I_MPI_FALLBACK=off 启动1024核任务，MPI init启动问题几乎消失了，MPI IO卡死问题几乎消失了。 但我的程序性能出现了巨大差异。 有时候1024核任务开始的500步迭代速度还可以，但随后变慢为1/3速度。有时候则一开始迭代速度就是预期的1/3慢。
我首先尝试理解内存的事情。 我首先给我的纯MPI程序添加了OpenMP directives，使其可以MPI/OpenMP混合并行。 然后测试了相同问题规模和相同输入参数下的内存占用，1024核任务。 纯MPI并行，调用了16个节点，每个节点64个MPI processes。 程序内部通过查看/proc/self/statm获知1024核总内存占用量为1.81E6 MB； MPI/OpenMP混合并行，调用16个节点，每个节点32个MPI processes，每个MPI process开2个threads，程序内部查看/proc/self/statm获知1024核总内存占用量为1.05E6 MB。 可以看到MPI并行本身的确占用了海量内存，几乎总内存占用量的一半都是MPI本身占用的。（这个事实，专业人员应该是了解的；我是逐渐认识到的，尝试解决问题。）
另外我检查了我的程序是否存在内存泄漏。 MPI模式的valgrind显示在调用的MPI calls中存在memory lost，鉴于Intel MPI的广泛使用，我认为这是false positive。valgrind没有找到我程序中的内存泄漏。
接下来，我尝试使用Intel自带的Intel Tracer And Collector来查看MPI性能。 但是按照Intel官方给的步骤加载ITAC，无法启动MPI。 所以为了理解我程序的MPI函数调用情况，我给我的程序手动添加了记录MPI调用时间的代码，每个进程记录自己的时间，不做额外的MPI call。 于是我分别提交了两次1024核纯MPI并行任务，得到了以下两图。 上图是前500步内程序运行较快时，单次迭代内1024核各自调用关键函数时的时间；下图则是程序慢3倍时，单次迭代内1024核各自调用关键函数时的时间。
上图中程序运行前500步内较快，但存在1024核负载不均衡、MPI通信部分节点延迟，但并不严重。 橘色线到绿色线是程序核心的MPI调用，也就是MPI_Isend/Irecv。 右端的粉红线到灰色线同样调用了MPI_Isend/Irecv。 下图中显然1024核中某些节点的MPI通信延迟非常高，而且均发生在调用MPI_Isend/Irecv的过程，也就是橘色线到绿色线、粉红线到灰色线。 所以1024核程序运行慢3倍的原因是某些节点MPI通信有问题，响应慢。
于是我测试了Intel自带的IMB-MPI1 Exchange。这个MPI Benchmark模式Exchange测试的是MPI_Isend/recv。 多次测试显示MPI_Isend/recv从128核，也就是2个节点，就出现性能极其严重的下降。 我理解64核1个节点采用shared memory模式，带宽就是会很高，但IB网络下节点间通信带宽不应该过分差。下见表格。" />
  <meta property="og:locale" content="en-us" />

  
    <style>body{-webkit-font-smoothing:antialiased;margin:0 20px}article{max-width:800px;margin-left:auto;margin-right:auto}a{color:#000;text-decoration:none}a:hover{font-weight:600;text-decoration:underline}.post-ads{margin:50px 0}.markdown-body{font-size:18px;max-width:100%}.markdown-body a{text-decoration:underline;text-decoration-color:#000}.markdown-body pre{padding:16px;overflow:auto;border-radius:10px}.markdown-body code{padding:.2em .4em;font-size:85%;background-color:#f6f8fa;border-radius:6px}.markdown-body pre>code{padding:0;font-size:100%;background-color:inherit;border:0}.Chinese .markdown-body{line-height:200%}.site-date-catalog{font-size:2rem}.header-title{font-size:2rem;font-weight:700;margin-top:32px}.header-title a{text-decoration:none}.header-subtitle{color:#666}.header-items{margin:10px 0}.header-item{margin:0 5px}.header-line{width:100%;border-width:2px;border-color:#482936;border-style:solid none none none}.lang-switch{font-weight:600}#posts-list{min-height:600px}.posts-line{font-size:1.2rem;margin:12px 0}.posts-categories{font-size:.8rem;margin:auto;text-align:center}.posts-category{padding:3px 0;border:#000 2px solid;border-radius:5px}.site-footer{margin-top:50px}.site-footer-item{margin-right:12px}.post-content img{max-width:100%;display:block;margin-right:auto;margin-top:12px}.post-header{margin-bottom:50px}.post-title{font-size:2rem;font-weight:600}.post-tags{display:inline;font-weight:600;padding:2px 5px;margin-right:6px;border:#000 2px solid;border-radius:5px}.post-date{font-weight:800;font-style:italic}.post-author{float:right;font-weight:600}.page-content{min-height:60%}.post-content{margin-bottom:50px}.post-content p{hyphens:auto;line-height:1.8;text-justify:ideographic;margin-bottom:1em}.related-content{border-width:3px;border-style:solid;border-color:#000;padding:0 10px;margin-bottom:50px;margin-top:100px}.related-content li{margin:5px 0}.taxonomy-term{font-size:3rem}.gallery-img{text-align:center}.gallery-img span{text-align:center}.gallery-img-desc{font-size:.8em;font-weight:800}#disqus_thread{position:relative}#disqus_thread:after{content:"";display:block;height:55px;width:100%;position:absolute;bottom:0;background:#fff}@media screen and (max-width:600px){.header-title,.header-subtitle,.header-items{text-align:center}.posts-line{font-size:16px}.markdown-body{font-size:16px}.post-title{font-size:2rem}.post-content p{letter-spacing:.05em}}@media screen and (max-width:48em){.posts-category{display:none}}</style>
  
  
    <style>.container,.container-fluid{margin-right:auto;margin-left:auto}.container-fluid{padding-right:2rem;padding-left:2rem}.row{box-sizing:border-box;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 1 auto;flex:initial;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-right:-.5rem;margin-left:-.5rem}.row.reverse{-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse}.col.reverse{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse}.col-xs,.col-xs-1,.col-xs-10,.col-xs-11,.col-xs-12,.col-xs-2,.col-xs-3,.col-xs-4,.col-xs-5,.col-xs-6,.col-xs-7,.col-xs-8,.col-xs-9,.col-xs-offset-0,.col-xs-offset-1,.col-xs-offset-10,.col-xs-offset-11,.col-xs-offset-12,.col-xs-offset-2,.col-xs-offset-3,.col-xs-offset-4,.col-xs-offset-5,.col-xs-offset-6,.col-xs-offset-7,.col-xs-offset-8,.col-xs-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-xs{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-xs-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-xs-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-xs-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-xs-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-xs-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-xs-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-xs-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-xs-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-xs-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-xs-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-xs-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-xs-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-xs-offset-0{margin-left:0}.col-xs-offset-1{margin-left:8.33333333%}.col-xs-offset-2{margin-left:16.66666667%}.col-xs-offset-3{margin-left:25%}.col-xs-offset-4{margin-left:33.33333333%}.col-xs-offset-5{margin-left:41.66666667%}.col-xs-offset-6{margin-left:50%}.col-xs-offset-7{margin-left:58.33333333%}.col-xs-offset-8{margin-left:66.66666667%}.col-xs-offset-9{margin-left:75%}.col-xs-offset-10{margin-left:83.33333333%}.col-xs-offset-11{margin-left:91.66666667%}.start-xs{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-xs{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-xs{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-xs{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-xs{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-xs{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-xs{-ms-flex-pack:distribute;justify-content:space-around}.between-xs{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-xs{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-xs{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}@media only screen and (min-width:48em){.container{width:49rem}.col-sm,.col-sm-1,.col-sm-10,.col-sm-11,.col-sm-12,.col-sm-2,.col-sm-3,.col-sm-4,.col-sm-5,.col-sm-6,.col-sm-7,.col-sm-8,.col-sm-9,.col-sm-offset-0,.col-sm-offset-1,.col-sm-offset-10,.col-sm-offset-11,.col-sm-offset-12,.col-sm-offset-2,.col-sm-offset-3,.col-sm-offset-4,.col-sm-offset-5,.col-sm-offset-6,.col-sm-offset-7,.col-sm-offset-8,.col-sm-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-sm{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-sm-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-sm-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-sm-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-sm-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-sm-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-sm-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-sm-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-sm-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-sm-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-sm-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-sm-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-sm-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-sm-offset-0{margin-left:0}.col-sm-offset-1{margin-left:8.33333333%}.col-sm-offset-2{margin-left:16.66666667%}.col-sm-offset-3{margin-left:25%}.col-sm-offset-4{margin-left:33.33333333%}.col-sm-offset-5{margin-left:41.66666667%}.col-sm-offset-6{margin-left:50%}.col-sm-offset-7{margin-left:58.33333333%}.col-sm-offset-8{margin-left:66.66666667%}.col-sm-offset-9{margin-left:75%}.col-sm-offset-10{margin-left:83.33333333%}.col-sm-offset-11{margin-left:91.66666667%}.start-sm{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-sm{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-sm{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-sm{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-sm{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-sm{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-sm{-ms-flex-pack:distribute;justify-content:space-around}.between-sm{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-sm{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-sm{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:64em){.container{width:65rem}.col-md,.col-md-1,.col-md-10,.col-md-11,.col-md-12,.col-md-2,.col-md-3,.col-md-4,.col-md-5,.col-md-6,.col-md-7,.col-md-8,.col-md-9,.col-md-offset-0,.col-md-offset-1,.col-md-offset-10,.col-md-offset-11,.col-md-offset-12,.col-md-offset-2,.col-md-offset-3,.col-md-offset-4,.col-md-offset-5,.col-md-offset-6,.col-md-offset-7,.col-md-offset-8,.col-md-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-md{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-md-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-md-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-md-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-md-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-md-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-md-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-md-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-md-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-md-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-md-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-md-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-md-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-md-offset-0{margin-left:0}.col-md-offset-1{margin-left:8.33333333%}.col-md-offset-2{margin-left:16.66666667%}.col-md-offset-3{margin-left:25%}.col-md-offset-4{margin-left:33.33333333%}.col-md-offset-5{margin-left:41.66666667%}.col-md-offset-6{margin-left:50%}.col-md-offset-7{margin-left:58.33333333%}.col-md-offset-8{margin-left:66.66666667%}.col-md-offset-9{margin-left:75%}.col-md-offset-10{margin-left:83.33333333%}.col-md-offset-11{margin-left:91.66666667%}.start-md{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-md{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-md{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-md{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-md{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-md{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-md{-ms-flex-pack:distribute;justify-content:space-around}.between-md{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-md{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-md{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}@media only screen and (min-width:75em){.container{width:76rem}.col-lg,.col-lg-1,.col-lg-10,.col-lg-11,.col-lg-12,.col-lg-2,.col-lg-3,.col-lg-4,.col-lg-5,.col-lg-6,.col-lg-7,.col-lg-8,.col-lg-9,.col-lg-offset-0,.col-lg-offset-1,.col-lg-offset-10,.col-lg-offset-11,.col-lg-offset-12,.col-lg-offset-2,.col-lg-offset-3,.col-lg-offset-4,.col-lg-offset-5,.col-lg-offset-6,.col-lg-offset-7,.col-lg-offset-8,.col-lg-offset-9{box-sizing:border-box;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:none;padding-right:.5rem;padding-left:.5rem}.col-lg{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-preferred-size:0;flex-basis:0;max-width:100%}.col-lg-1{-ms-flex-preferred-size:8.33333333%;flex-basis:8.33333333%;max-width:8.33333333%}.col-lg-2{-ms-flex-preferred-size:16.66666667%;flex-basis:16.66666667%;max-width:16.66666667%}.col-lg-3{-ms-flex-preferred-size:25%;flex-basis:25%;max-width:25%}.col-lg-4{-ms-flex-preferred-size:33.33333333%;flex-basis:33.33333333%;max-width:33.33333333%}.col-lg-5{-ms-flex-preferred-size:41.66666667%;flex-basis:41.66666667%;max-width:41.66666667%}.col-lg-6{-ms-flex-preferred-size:50%;flex-basis:50%;max-width:50%}.col-lg-7{-ms-flex-preferred-size:58.33333333%;flex-basis:58.33333333%;max-width:58.33333333%}.col-lg-8{-ms-flex-preferred-size:66.66666667%;flex-basis:66.66666667%;max-width:66.66666667%}.col-lg-9{-ms-flex-preferred-size:75%;flex-basis:75%;max-width:75%}.col-lg-10{-ms-flex-preferred-size:83.33333333%;flex-basis:83.33333333%;max-width:83.33333333%}.col-lg-11{-ms-flex-preferred-size:91.66666667%;flex-basis:91.66666667%;max-width:91.66666667%}.col-lg-12{-ms-flex-preferred-size:100%;flex-basis:100%;max-width:100%}.col-lg-offset-0{margin-left:0}.col-lg-offset-1{margin-left:8.33333333%}.col-lg-offset-2{margin-left:16.66666667%}.col-lg-offset-3{margin-left:25%}.col-lg-offset-4{margin-left:33.33333333%}.col-lg-offset-5{margin-left:41.66666667%}.col-lg-offset-6{margin-left:50%}.col-lg-offset-7{margin-left:58.33333333%}.col-lg-offset-8{margin-left:66.66666667%}.col-lg-offset-9{margin-left:75%}.col-lg-offset-10{margin-left:83.33333333%}.col-lg-offset-11{margin-left:91.66666667%}.start-lg{-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;text-align:start}.center-lg{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;text-align:center}.end-lg{-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;text-align:end}.top-lg{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.middle-lg{-webkit-box-align:center;-ms-flex-align:center;align-items:center}.bottom-lg{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.around-lg{-ms-flex-pack:distribute;justify-content:space-around}.between-lg{-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between}.first-lg{-webkit-box-ordinal-group:0;-ms-flex-order:-1;order:-1}.last-lg{-webkit-box-ordinal-group:2;-ms-flex-order:1;order:1}}</style>
  

  <link rel="stylesheet" href="/blog/css/extra.css">

  <link href="/blog/index.xml" rel="alternate" type="application/rss+xml"
    title="石京昶的博客">
  
  
  
  
  

  
  <style> @import url('https://fonts.loli.net/css2?family=Noto+Sans+SC&Noto+Serif+SC&Fira+Code'); </style>

</head>


<body>
  <article class="post " id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="header-title">
    <a href="/blog"
      >石京昶的博客</a
    >
  </div>
  <div class="header-subtitle"></div>
</header>
<div class="row end-md center-xs header-items">
  
  <div class="header-item">
    <a href="https://shijingchang.cn" target="_blank">About</a>
  </div>
  
  <div class="header-item">
    <a href="/blog/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="header-item">
    <a href="https://github.com/joway/hugo-theme-yinyang" target="_blank">Theme</a>
  </div>
  
</div>
<div class="row end-xs">
   
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">调整Intel MPI 2018中的参数提升通信性能</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2021-10-01 15:29:51 CST">
                01 Oct 2021
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://shijingchang.cn">@石京昶</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <h1 id="概览">概览</h1>
<p>前些天在超算上跑我的程序，结果遇到了一些MPI相关问题，这里记录我解决（某种程度上）这个问题的过程。</p>
<h1 id="背景">背景</h1>
<p>去年12月-今年5月，我使用B地超算，跑我自己写的纯MPI并行程序，使用<code>mpi/Intel/20.4.3</code>，1024核以下没有出现过启动不了的情况。
当时1024核常常无法启动<code>MPI_Init</code>，管理员当时承认确实有问题但无法解决。
后续我一直在用<code>mpi/intel/18.0.2</code>。</p>
<p>8月我再次开始使用该超算，<code>mpi/intel/18.0.2</code>跑我的纯MPI并行程序。
但是出现了512核也时常<code>MPI collective IO</code>卡死的问题。
月初无法解决此问题，于是尝试该地超算其它分区以及J地的超算。</p>
<h1 id="解决">解决</h1>
<p>J地超算上提交几次1000+核任务均无法启动<code>MPI_Init</code>，遂放弃。</p>
<p>B地超算上新分区节点任务繁忙，1000+核的任务等待时间较长，所以我没有太多测试。
但几次测试时常出现运行时MPI卡死。
经过阅读MPI相关资料，发现在Intel MPI 18版本下添加环境变量<code>I_MPI_DAPL_UD=on</code>启动UD模式通信可有效规避此问题。
回到B地原分区下，添加此环境变量后同样使得MPI卡死的问题几乎消失。
Intel官方文档解释：UD模式有效减少了MPI通信<code>send/recv</code>所需的缓冲区，显著降低了MPI占用的内存。
所以之前遇到的1000核任务运行时MPI卡死的现象应该是与内存相关。</p>
<p>鉴于三个分区的表现没有本质差别，我又回到了B地原分区尝试解决MPI问题。
该分区采用<code>mpi/intel/18.0.2</code>，设定以下环境变量</p>
<pre><code>I_MPI_FABRICS=shm:dapl
I_MPI_DAPL_UD=on
I_MPI_FALLBACK=off
</code></pre><p>启动1024核任务，<code>MPI init</code>启动问题几乎消失了，<code>MPI IO</code>卡死问题几乎消失了。
但我的程序性能出现了巨大差异。
有时候1024核任务开始的500步迭代速度还可以，但随后变慢为1/3速度。有时候则一开始迭代速度就是预期的1/3慢。</p>
<p>我首先尝试理解内存的事情。
我首先给我的纯MPI程序添加了<code>OpenMP directives</code>，使其可以MPI/OpenMP混合并行。
然后测试了相同问题规模和相同输入参数下的内存占用，1024核任务。
纯MPI并行，调用了16个节点，每个节点64个MPI processes。
程序内部通过查看<code>/proc/self/statm</code>获知1024核总内存占用量为<code>1.81E6 MB</code>；
MPI/OpenMP混合并行，调用16个节点，每个节点32个MPI processes，每个MPI process开2个threads，程序内部查看<code>/proc/self/statm</code>获知1024核总内存占用量为<code>1.05E6 MB</code>。
可以看到MPI并行本身的确占用了海量内存，几乎总内存占用量的一半都是MPI本身占用的。（这个事实，专业人员应该是了解的；我是逐渐认识到的，尝试解决问题。）</p>
<p>另外我检查了我的程序是否存在内存泄漏。
MPI模式的<code>valgrind</code>显示在调用的MPI calls中存在memory lost，鉴于Intel MPI的广泛使用，我认为这是false positive。valgrind没有找到我程序中的内存泄漏。</p>
<p>接下来，我尝试使用Intel自带的Intel Tracer And Collector来查看MPI性能。
但是按照Intel官方给的步骤加载ITAC，无法启动MPI。
所以为了理解我程序的MPI函数调用情况，我给我的程序手动添加了记录MPI调用时间的代码，每个进程记录自己的时间，不做额外的MPI call。
于是我分别提交了两次1024核纯MPI并行任务，得到了以下两图。
上图是前500步内程序运行较快时，单次迭代内1024核各自调用关键函数时的时间；下图则是程序慢3倍时，单次迭代内1024核各自调用关键函数时的时间。</p>
<p><img src="mpi_18_default_normal_running.png" alt="Intel MPI 2018默认参数下正常运行时1024进程各自的时间表"></p>
<p><img src="mpi_18_default_slow_running.png" alt="Intel MPI 2018默认参数下极慢运行时1024进程各自的时间表"></p>
<p>上图中程序运行前500步内较快，但存在1024核负载不均衡、MPI通信部分节点延迟，但并不严重。
橘色线到绿色线是程序核心的MPI调用，也就是<code>MPI_Isend/Irecv</code>。
右端的粉红线到灰色线同样调用了<code>MPI_Isend/Irecv</code>。
下图中显然1024核中某些节点的MPI通信延迟非常高，而且均发生在调用<code>MPI_Isend/Irecv</code>的过程，也就是橘色线到绿色线、粉红线到灰色线。
所以1024核程序运行慢3倍的原因是某些节点MPI通信有问题，响应慢。</p>
<p>于是我测试了Intel自带的<code>IMB-MPI1 Exchange</code>。这个MPI Benchmark模式Exchange测试的是<code>MPI_Isend/recv</code>。
多次测试显示<code>MPI_Isend/recv</code>从128核，也就是2个节点，就出现性能极其严重的下降。
我理解64核1个节点采用shared memory模式，带宽就是会很高，但IB网络下节点间通信带宽不应该过分差。下见表格。</p>
<p><img src="BSCC-A2_IntelMPI2018_DefaultParameters.png" alt="Intel MPI 2018默认参数"></p>
<p><img src="BSCC-A2_IntelMPI2021_DefaultParameters.png" alt="Intel MPI 2021默认参数"></p>
<p><img src="BSCC-A2_IntelMPI2018_ImprovedParameters.png" alt="Intel MPI 2018调优参数"></p>
<p>我测试了三种：<code>mpi/intel/18.0.2</code>（默认参数），<code>mpi/intel/20.4.3</code>（默认参数），<code>mpi/intel/18.0.2</code>（调整参数）。
从上面3个表格中可以看到<code>Intel MPI 20.4.3</code>版本（多次尝试后成功启动的一次）性能合乎IB网络预期，可以达到<code>4Gb/s</code>。
<code>Intel MPI 18.0.2</code>默认参数下2节点的带宽就掉到了不到<code>600Mb/s</code>。
而通过增大<code>I_MPI_EAGER_THRESHOLD, I_MPI_DAPL_UD_DIRECT_COPY_THRESHOLD</code>，多节点带宽可以<code>2Gb/s~3Gb/s</code>，但没达到<code>Intel MPI 20.4.3</code>默认参数下的<code>4Gb/s</code>。</p>
<p><code>I_MPI_EAGER_THRESHOLD</code>是MPI在message size较小时采用eager模式，message size较大时采用rendezvous模式。
两个模式的区别在于rendezvous模式中Sender要等Receiver返回消息说它准备好接收数据时才会发送。
Receiver准备好接收数据也就意味着Receiver给接收的buffer分配好了内存。
<code>I_MPI_DAPL_UD_DIRECT_COPY_THRESHOLD</code>也是类似的操作。Intel官方解释是</p>
<pre><code>Set this environment variable to control the DAPL UD direct-copy protocol threshold. Data transfer algorithms for the DAPL-capable network fabrics are selected based on the following scheme:

- Messages shorter than or equal to &lt;nbytes&gt; are sent using the eager protocol through the internal pre-registered buffers. This approach is faster for short messages.
- Messages larger than &lt;nbytes&gt; are sent using the direct-copy protocol. It does not use any buffering but involves registration of memory on sender and receiver sides. This approach is faster for large messages.
</code></pre><p>我没有继续测试到底是以上三个参数中哪一个起到了关键作用，但我倾向于<code>I_MPI_DAPL_UD_DIRECT_COPY_THRESHOLD</code>。
不管是哪个参数，均涉及了<code>pre-registered buffers</code>。
并且，提高阈值使这个<code>pre-registered buffers</code>减少可以提高通信带宽。</p>
<p>所以我针对多节点MPI通讯性能差的问题，提出一个想法是：B地超算该分区的节点在内存、page cache设定上可能存在问题，MPI send/recv数据过程中使用buffer要向kernel申请内存，而kernel为了给出内存，需要反复回收page cache等，同时NUMA架构下，分配的内存与CPU没有就近相连，导致的性能巨大损失。
但这只是个不成熟的猜想，我并非科班出身。对这个猜想的验证，也留待以后工作中注意。</p>
<p>我在我们课题组几百核的小集群验证了：Linux系统在内存回收、page cache上的设定不合理导致的某计算流体力学商业软件的2倍性能损失。
见<a href="https://shijingchang.cn/blog/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster/">解决小集群中某节点比其它节点慢的问题</a>和<a href="https://shijingchang.cn/blog/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster_conti/">解决小集群中某节点比其它节点慢的问题（续）</a>。</p>
<p>采用Intel MPI 2018的调优参数运行我的纯MPI程序，1024核运行，各进程时间戳如下</p>
<p><img src="mpi_18_improved.png" alt="Intel MPI 2018调优参数下运行时1024进程各自的时间表"></p>
<p>可以看到单个迭代步共<code>0.3s</code>，某些节点仍然出现将近<code>0.1s</code>的通信延迟，同时MPI load imbalance比之前Intel MPI 2018默认参数下减轻了很多。
这个将近<code>0.1s</code>的通信延迟呈现了某种规律，也许可以进一步调优。
但这个问题留待以后解决，现在的任务是尽快解决当前的课题。</p>

        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        
          <div class="row">
            <div class="col-xs-12">
              
            </div>
          </div>

          



          
          
          <div style="height: 50px;"></div>
          
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  

<script>
  
  
    
    
  
</script>

  

</body>

</html>
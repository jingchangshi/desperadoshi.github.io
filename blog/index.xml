<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>石京昶的博客</title>
    <link>https://shijingchang.cn/</link>
    <description>Recent content on 石京昶的博客</description>
    <image>
      <url>https://shijingchang.cn/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://shijingchang.cn/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 13 Jan 2022 16:45:57 +0800</lastBuildDate><atom:link href="https://shijingchang.cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>西北工业大学博士/硕士论文LaTeX模板-Windows精简版</title>
      <link>https://shijingchang.cn/posts/npu_thesis_latex_template/</link>
      <pubDate>Thu, 13 Jan 2022 16:45:57 +0800</pubDate>
      
      <guid>https://shijingchang.cn/posts/npu_thesis_latex_template/</guid>
      <description>说明 西北工业大学博士/硕士论文LaTeX模板，针对Windows平台集成TinyTeX引擎套装、中文字体、编译本模板所需宏包、SumatraPDF阅读器（支持热更新），总共232MB，无需安装几个GB的完整TeXLive，方便小白使用。
下载 从Github或Gitee下载。
参考 LaTeX模板基于Yet-Another-LaTeX-Template-for-NPU-Thesis并添加修改而成。</description>
    </item>
    
    <item>
      <title>校园网内小集群自动重连WiFi并自动发送新IP到指定邮箱</title>
      <link>https://shijingchang.cn/posts/auto_reconnect_wifi_send_ip/</link>
      <pubDate>Thu, 13 Jan 2022 16:22:32 +0800</pubDate>
      
      <guid>https://shijingchang.cn/posts/auto_reconnect_wifi_send_ip/</guid>
      <description>问题描述 我瓜西北工业大学长安校区内开通了WiFi全覆盖，其中NWPU-FREE无需账号登录，可以用于实验区中小计算集群联网。 用户可以在宿舍、学院楼ssh登录小计算集群。 当然，也可以从校外使用VPN进入校内网络，进而连接这些小集群。 但是存在一个非常不方便的问题： 校园网采用DHCP方式分配IP。 在一个不确定的时间间隔后，小集群的WiFi连接会断开（此时WiFi保持了“虚假”的连接），IP有时候也会变。
解决方案 一种方式是向学校的信息中心申请分配静态IP，保证连接后IP不变。但很有可能WiFi还会出现假连接真断开的情况。
另一种方式是写一个shell脚本，自动检测WiFi是否真的连接，如果已经断开，则自动重连并发送新IP到指定邮箱。 具体如下。
首先设定crontab，每隔1小时执行脚本，检测WiFi并自动重连。
1 * * * * /usr/bin/bash /path/to/auto_connect_wifi.sh auto_connect_wifi.sh的内容如下：
#!/bin/bash# Gateway/DHCP serverdhcp_server=10.27.0.1wifi_name=&amp;#34;NWPU-FREE&amp;#34;nic_name=&amp;#34;wlp2s0u2&amp;#34;test_website=&amp;#34;baidu.com&amp;#34;#ping_ret=$(ping -c 5 ${dhcp_server} | tail -n 2 | sed -n &amp;#39;1p&amp;#39; | awk &amp;#39;{print $4}&amp;#39;)if [ &amp;#34;$ping_ret&amp;#34; == &amp;#34;&amp;#34; ]; thennmcli d wifi connect ${wifi_name} ifname ${nic_name}sleep 10ping -c 5 ${test_website}bash /path/to/send_ip.shelif [ &amp;#34;$ping_ret&amp;#34; == &amp;#34;0&amp;#34; ]; then# ping failsecho &amp;#34;$(date): WiFi disconnected!</description>
    </item>
    
    <item>
      <title>调整Intel MPI 2018中的参数提升通信性能</title>
      <link>https://shijingchang.cn/posts/adjust_intel_mpi_parameters_to_improve_performance/</link>
      <pubDate>Fri, 01 Oct 2021 15:29:51 +0800</pubDate>
      
      <guid>https://shijingchang.cn/posts/adjust_intel_mpi_parameters_to_improve_performance/</guid>
      <description>概览 前些天在超算上跑我的程序，结果遇到了一些MPI相关问题，这里记录我解决（某种程度上）这个问题的过程。
背景 去年12月-今年5月，我使用B地超算，跑我自己写的纯MPI并行程序，使用mpi/Intel/20.4.3，1024核以下没有出现过启动不了的情况。 当时1024核常常无法启动MPI_Init，管理员当时承认确实有问题但无法解决。 后续我一直在用mpi/intel/18.0.2。
8月我再次开始使用该超算，mpi/intel/18.0.2跑我的纯MPI并行程序。 但是出现了512核也时常MPI collective IO卡死的问题。 月初无法解决此问题，于是尝试该地超算其它分区以及J地的超算。
解决 J地超算上提交几次1000+核任务均无法启动MPI_Init，遂放弃。
B地超算上新分区节点任务繁忙，1000+核的任务等待时间较长，所以我没有太多测试。 但几次测试时常出现运行时MPI卡死。 经过阅读MPI相关资料，发现在Intel MPI 18版本下添加环境变量I_MPI_DAPL_UD=on启动UD模式通信可有效规避此问题。 回到B地原分区下，添加此环境变量后同样使得MPI卡死的问题几乎消失。 Intel官方文档解释：UD模式有效减少了MPI通信send/recv所需的缓冲区，显著降低了MPI占用的内存。 所以之前遇到的1000核任务运行时MPI卡死的现象应该是与内存相关。
鉴于三个分区的表现没有本质差别，我又回到了B地原分区尝试解决MPI问题。 该分区采用mpi/intel/18.0.2，设定以下环境变量
I_MPI_FABRICS=shm:dapl I_MPI_DAPL_UD=on I_MPI_FALLBACK=off 启动1024核任务，MPI init启动问题几乎消失了，MPI IO卡死问题几乎消失了。 但我的程序性能出现了巨大差异。 有时候1024核任务开始的500步迭代速度还可以，但随后变慢为1/3速度。有时候则一开始迭代速度就是预期的1/3慢。
我首先尝试理解内存的事情。 我首先给我的纯MPI程序添加了OpenMP directives，使其可以MPI/OpenMP混合并行。 然后测试了相同问题规模和相同输入参数下的内存占用，1024核任务。 纯MPI并行，调用了16个节点，每个节点64个MPI processes。 程序内部通过查看/proc/self/statm获知1024核总内存占用量为1.81E6 MB； MPI/OpenMP混合并行，调用16个节点，每个节点32个MPI processes，每个MPI process开2个threads，程序内部查看/proc/self/statm获知1024核总内存占用量为1.05E6 MB。 可以看到MPI并行本身的确占用了海量内存，几乎总内存占用量的一半都是MPI本身占用的。（这个事实，专业人员应该是了解的；我是逐渐认识到的，尝试解决问题。）
另外我检查了我的程序是否存在内存泄漏。 MPI模式的valgrind显示在调用的MPI calls中存在memory lost，鉴于Intel MPI的广泛使用，我认为这是false positive。valgrind没有找到我程序中的内存泄漏。
接下来，我尝试使用Intel自带的Intel Tracer And Collector来查看MPI性能。 但是按照Intel官方给的步骤加载ITAC，无法启动MPI。 所以为了理解我程序的MPI函数调用情况，我给我的程序手动添加了记录MPI调用时间的代码，每个进程记录自己的时间，不做额外的MPI call。 于是我分别提交了两次1024核纯MPI并行任务，得到了以下两图。 上图是前500步内程序运行较快时，单次迭代内1024核各自调用关键函数时的时间；下图则是程序慢3倍时，单次迭代内1024核各自调用关键函数时的时间。
上图中程序运行前500步内较快，但存在1024核负载不均衡、MPI通信部分节点延迟，但并不严重。 橘色线到绿色线是程序核心的MPI调用，也就是MPI_Isend/Irecv。 右端的粉红线到灰色线同样调用了MPI_Isend/Irecv。 下图中显然1024核中某些节点的MPI通信延迟非常高，而且均发生在调用MPI_Isend/Irecv的过程，也就是橘色线到绿色线、粉红线到灰色线。 所以1024核程序运行慢3倍的原因是某些节点MPI通信有问题，响应慢。
于是我测试了Intel自带的IMB-MPI1 Exchange。这个MPI Benchmark模式Exchange测试的是MPI_Isend/recv。 多次测试显示MPI_Isend/recv从128核，也就是2个节点，就出现性能极其严重的下降。 我理解64核1个节点采用shared memory模式，带宽就是会很高，但IB网络下节点间通信带宽不应该过分差。下见表格。</description>
    </item>
    
    <item>
      <title>解决小集群中某节点比其它节点慢的问题（续）</title>
      <link>https://shijingchang.cn/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster_conti/</link>
      <pubDate>Thu, 01 Jul 2021 22:37:20 +0800</pubDate>
      
      <guid>https://shijingchang.cn/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster_conti/</guid>
      <description>来源 在上篇文章中，之前 NUMA 配置不当导致小集群上计算节点跑某 CFD 商业软件比预期慢一倍， 临时的解决方案是强制清除page cache：echo 1 &amp;gt; /proc/sys/vm/drop_caches. 但希望修改系统设定，使得 NUMA 设定适用于 CFD 场景。
解决 某 CFD 商业软件跑满计算节点 40 核并且跑得慢的时候其使用 NUMA 架构内存情况如下图
图中可以看到在 CPU 0 上的进程占用的内存有一半放在了 CPU 1 对应的内存上。 这导致了整体性能下降。
为何系统会让 CPU 0 上的进程同时使用 CPU 0 和 CPU 1 上的内存？而 CPU 1 上的进程只使用 CPU 1 上的内存。 这个问题并不会发生在 page cache 被清空后立刻提交的 40 核计算任务上，只有 page cache 足够大，导致 free 不够的时候。 所以这是一个系统回收内存的策略设定问题。
Linux 系统回收 NUMA 架构下内存的设定被一个重要参数左右，这就是 zone_reclaim_mode。
Zone_reclaim_mode allows someone to set more or less aggressive approaches to reclaim memory when a zone runs out of memory.</description>
    </item>
    
    <item>
      <title>解决小集群中某节点比其它节点慢的问题</title>
      <link>https://shijingchang.cn/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster/</link>
      <pubDate>Fri, 18 Jun 2021 10:02:20 +0800</pubDate>
      
      <guid>https://shijingchang.cn/posts/resolve_issue_of_poor_performance_of_one_node_in_a_cluster/</guid>
      <description>来源 群里 H 老师报告 node7 上跑某著名 CFD 商业软件比其它节点慢一倍以上，问我有没有类似经历。
测试 我来测试一下我的 NFR 程序。结果发现确实 node7 上比其它节点慢，如下图
node7 上 NFR 跑完需要 76 秒， node4 上跑完需要 61.5 秒。 所以 node7 慢的表现是某著名 CFD 商业软件慢一倍以上，我的 NFR 慢 24%。
但是单核跑 NFR 在 node7 和 node4 上没有性能差异。
解决 Linux 上性能分析参考 Brendan D. Gregg 大神的网站。 了解到使用 perf 给出性能统计报告，也就是上面的图。perf 使用参考 Brendan D. Gregg 大神的网页。 perf 可以直接统计 MPI 并行程序的性能。
首先使用perf stat -d mpirun -n 40 prog input.file发现 node7 上 NFR 的 page-faults 是 node4 上的 3 倍。</description>
    </item>
    
  </channel>
</rss>
